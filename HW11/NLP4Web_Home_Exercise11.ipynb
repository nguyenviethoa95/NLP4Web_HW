{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and the Web: Exercise 11\n",
    "\n",
    "So far, you have learned how to vectorize text, how to train classifiers and how to do IR. In this exercise we look at a more practical use case: Community Question Answering (cQA). \n",
    "\n",
    "The web is full of rich ressources where humans can post questions and answers to specific topics (e.g. https://stackexchange.com/, https://quora.com or https://answers.yahoo.com/). In many cases the information need of a user is not entirely new, but a similar question has already been asked and answered in some form.\n",
    "\n",
    "\n",
    "The data is a small sample of the SemEval2015 Task 3. The data comes from a Qatar Living Forum.  This subset comes as `\\t` seperated file and includes multiple columns:\n",
    "* **qid** is the unique ID for each question\n",
    "* **cid** is the unique ID for each comment\n",
    "* **question_category** is the category of the question (such as \"Beauty and Style\")\n",
    "* **question_subject** is the subject associated with a question\n",
    "* **question** is the textual question\n",
    "* **comment** is a comment for this question\n",
    "* **comment_gold** is the label, whether this comment is a \"good\" or \"bad\" answer to the question\n",
    "\n",
    "Each question can come with multiple comments; in this case, a new row is used for each differing comment (but the same question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - 2 Points\n",
    "\n",
    "**a)** Load the provided train and dev set. Lowercase the fields `question_subject`, `question` and `comment` of both data splits. After lowercasing, output the first five elements (e.g. `head()`) of the training split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/train.tsv\",sep=\"\\t\", header=0)\n",
    "test_data = pd.read_csv(\"data/dev.tsv\",sep=\"\\t\", header=0)\n",
    "\n",
    "# Lowercase the field\n",
    "train_data[\"question_subject\"] = train_data[\"question_subject\"].apply(lambda x: str(x).lower())\n",
    "train_data[\"question\"]= train_data[\"question\"].apply(lambda x: str(x).lower())\n",
    "train_data[\"comment\"]= train_data[\"comment\"].apply(lambda x: str(x).lower())\n",
    "\n",
    "test_data[\"question_subject\"] = test_data[\"question_subject\"].apply(lambda x: str(x).lower())\n",
    "test_data[\"question\"]= test_data[\"question\"].apply(lambda x: str(x).lower())\n",
    "test_data[\"comment\"]= test_data[\"comment\"].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Create three new columns in the training and development splits: `question_vectors`, `subject_vectors`, `comment_vectors`. Use spaCy to convert each token of the fields `question`, `question_subject`, `comment` into a dense word vector (`token.vector`) and store these word vectors in the new columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(sentence):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        sentence: list of words\n",
    "    return:\n",
    "        result: (nummer of words, 300) matrix of vectorized tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = nlp.tokenizer\n",
    "    tokens = tokenizer(sentence)\n",
    "    \n",
    "    # Initialize output matrix\n",
    "    vectorized_sent = np.zeros(shape=(len(tokens),300))\n",
    "    \n",
    "    for i, tok in enumerate(tokens):\n",
    "        vectorized_token = nlp(tok.text).vector\n",
    "        vectorized_sent[i] = vectorized_token\n",
    "        \n",
    "    return vectorized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the field\n",
    "train_data[\"question_vectors\"] = train_data[\"question_subject\"].apply(sent2vec)\n",
    "train_data[\"subject_vectors\"] = train_data[\"question\"].apply(sent2vec)\n",
    "train_data[\"comment_vectors\"] = train_data[\"comment\"].apply(sent2vec)\n",
    "\n",
    "test_data[\"question_vectors\"] = test_data[\"question_subject\"].apply(sent2vec)\n",
    "test_data[\"subject_vectors\"] = test_data[\"question\"].apply(sent2vec)\n",
    "test_data[\"comment_vectors\"] = test_data[\"comment\"].apply(sent2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing word takes a long time. We will create a pickle file out of the vectors\n",
    "\n",
    "train_data.to_pickle(\"data/train.pkl\")\n",
    "test_data.to_pickle(\"data/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset vectors we vectorized from earlier\n",
    "train_data_ = pd.read_pickle(\"data/train.pkl\")\n",
    "test_data_ = pd.read_pickle(\"data/test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - 8 Points\n",
    "You will train a classifier to predict whether a comment is a good answer or not, based on the word vectors. You will explore different strategies\n",
    "* how to compute a single fixed-length vector out of the word vectors\n",
    "* how to combine these single vectors from different fields (such as `question` and `comment`)\n",
    " \n",
    "**a)** Implement the function `embedding_fn_max_pooling(word_vectors)`. It should compute a single vector (of the same dimensionality as a single word vector) via max pooling: In each $i$th dimension, the resulting representation must have the maximum value based on all $i$th dimension values from all words.\n",
    "\n",
    "Example:\n",
    "\n",
    "$w_0 = [0.1, 0.2, 0.3, 0.4]$ \n",
    "\n",
    "$w_1 = [1.0, -1.5, 2.0, -2.5]$\n",
    "\n",
    "The resulting embedding should be $e = [max(0.1, 1.0), max(0.2, -1.5), max(0.3, 2.0), max(0.4, -2.5)] = [1.0, 0.2, 2.0, 0.4]$.\n",
    "\n",
    "Output the dimensionality of the resulting embedding after applying this function on the word vectors of a single question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_fn_max_pooling(word_vectors):\n",
    "    \"\"\"\n",
    "    Converts an arbitrary number of d-dimensional word vectors \n",
    "    into a single d-dimensional embedding via max-pooling.\n",
    "    :param word_vectors\n",
    "        list of d-dimensional word vectors (one vector for each token)\n",
    "    :returns\n",
    "        d-dimensional embedding\n",
    "    \"\"\"\n",
    "    sen_vec =  np.amax(word_vectors, axis=0)\n",
    "    return sen_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17 15 16]\n"
     ]
    }
   ],
   "source": [
    "arr2D = np.array([[11, 12, 13],\n",
    "                     [14, 15, 16],\n",
    "                     [17, 15, 11],\n",
    "                     [12, 14, 15]])\n",
    "print(embedding_fn_max_pooling(arr2D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Implement the function `feature_fn_concatenate_question_comment(df, embedding_fn)`. It should\n",
    "* use the `embedding_fn` to create fixed-length vectors from the fields `question_vectors` and `comment_vectors` individually. (For each sample: one vector for the question, one vector for the comments).\n",
    "* Concatenate both vectors\n",
    "* Return these concatenated vectors (the features) for all samples in the dataframe `df`.\n",
    "\n",
    "You will use these vectors to train a new classifier. Execute this function using `embedding_fn=embedding_fn_max_pooling` on the train split and output the shape of the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_fn_concatenate_question_comment(df, embedding_fn):\n",
    "    \"\"\"\n",
    "    Uses the embedding_fn to create d-dimensional embeddings from the tokens \n",
    "    of the questions and comments respectively, and concatenates these embeddings.\n",
    "    :param df\n",
    "        Dataframe consisting of multiple samples. Features will be computed for each sample individually\n",
    "    :param embedding_fn\n",
    "        As in 2a)\n",
    "    :returns\n",
    "        Matrix of shape (n, d) whereas \n",
    "        n is the number of samples in df and \n",
    "        d is the output dimensionality of the embedding_fn\n",
    "    \"\"\"\n",
    "    vectorized_qc = np.zeros(shape=(df.shape[0], 300*2))\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        q_v = embedding_fn(row[\"question_vectors\"])\n",
    "        c_v = embedding_fn(row[\"comment_vectors\"])\n",
    "        vectorized_qc[i] = np.concatenate([q_v,c_v])\n",
    "    \n",
    "    return vectorized_qc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Execute the (already implemented) function `train_and_evaluate` to train a new classifier based on the two functions you implemented. Use `embedding_fn=embedding_fn_max_pooling`, `feature_fn=feature_fn_concatenate_question_comment` and leave the classifier as the default parameter. The function will normalize the computed features and train and evaluate a support vector machine.\n",
    "\n",
    "What is the advantage of using F1 macro (as opposed to accuracy or F1 micro/weighted) when we want to weight each label equally?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.61      0.21      0.31        92\n",
      "        Good       0.81      0.96      0.88       322\n",
      "\n",
      "    accuracy                           0.79       414\n",
      "   macro avg       0.71      0.58      0.59       414\n",
      "weighted avg       0.77      0.79      0.75       414\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(df_train, df_dev, embedding_fn, feature_fn, \n",
    "                      classifier=make_pipeline(StandardScaler(), SVC())):\n",
    "    \n",
    "    # 1) Compute vectors\n",
    "    X_train = feature_fn(df_train, embedding_fn)\n",
    "    X_dev = feature_fn(df_dev, embedding_fn)\n",
    "    \n",
    "    # 2) Train classifier\n",
    "    classifier.fit(X_train, df_train['comment_gold'])\n",
    "    \n",
    "    # 3) Predict dev data\n",
    "    predictions = classifier.predict(X_dev)\n",
    "    \n",
    "    # 4) Compute and output metrics\n",
    "    print(classification_report(df_dev['comment_gold'], predictions))\n",
    "\n",
    "    \n",
    "# You probably have to update the names of your train and dev set\n",
    "train_and_evaluate(train_data_, test_data_, embedding_fn_max_pooling, feature_fn_concatenate_question_comment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With accuracy, each sample is equally contributing to the final metric. Hence, if one label is highly represented in the data, the prediction based on this label will largely dominate the final accuracy score. In f1-macro, every label is weighted equally, regardless on how many samples each label represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Create\n",
    "* one new embedding function (as in 2a) that converts multiple word vectors into a single fixed-length vector (e.g. by averaging over all word embeddings)\n",
    "* two new feature functions (as in 2b), that combine these vectors of different fields to generate the final features (e.g. add different columns, use fewer columns, use average instead of concatenating, ...).\n",
    "\n",
    "Use self-explanatory function names or add a comment to describe what each function does.\n",
    "\n",
    "Run a grid search (parts are already implemented) for all combinations of all two embedding functions and all three feature functions. Which combination yields in the highest F1 macro? Explain in up to three sentences how the task of *question similarity* comes into play, when you want to create a basic cQA system with this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "\n",
    "Highest f1-macro is yield from the combination of averaging embedding with concat-question-comment feature (68%)\n",
    "\n",
    "We can use this system to give answers for new incoming questions. For example, we choose the a set of answers from the samw topic of the question. Then we perform the prediction using the model on the choosen set of answers and the given question to determine the labels of the combinations of question and answer. We then return the answer which is labeled as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_fn_average(word_vectors):\n",
    "    \"\"\"\n",
    "    Converts an arbitrary number of d-dimensional word vectors \n",
    "    into a single d-dimensional embedding via averaging.\n",
    "    :param word_vectors\n",
    "        list of d-dimensional word vectors (one vector for each token)\n",
    "    :returns\n",
    "        d-dimensional embedding\n",
    "    \"\"\"\n",
    "    sen_vec =  np.mean(word_vectors, axis=0)\n",
    "\n",
    "    return sen_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_fn_averaging_question_comment(df, embedding_fn):\n",
    "    \"\"\"\n",
    "    Uses the embedding_fn to create d-dimensional embeddings from the tokens \n",
    "    of the questions and comments respectively, and averagibg these embeddings.\n",
    "    :param df\n",
    "        Dataframe consisting of multiple samples. Features will be computed for each sample individually\n",
    "    :param embedding_fn\n",
    "        As in 2a)\n",
    "    :returns\n",
    "        Matrix of shape (n, d) whereas \n",
    "        n is the number of samples in df and \n",
    "        d is the output dimensionality of the embedding_fn\n",
    "    \"\"\"\n",
    "    vectorized_qc = np.zeros(shape=(df.shape[0], 300*2))\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        q_v = embedding_fn(row[\"question_vectors\"])\n",
    "        c_v = embedding_fn(row[\"comment_vectors\"])\n",
    "        vectorized_qc[i] = np.mean([q_v,c_v])\n",
    "    \n",
    "    return vectorized_qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_fn_concatenate_question_comment_subject(df, embedding_fn):\n",
    "    \"\"\"\n",
    "    Uses the embedding_fn to create d-dimensional embeddings from the tokens \n",
    "    of the questions, comments and subject respectively, and concatenates these embeddings.\n",
    "    :param df\n",
    "        Dataframe consisting of multiple samples. Features will be computed for each sample individually\n",
    "    :param embedding_fn\n",
    "        As in 2a)\n",
    "    :returns\n",
    "        Matrix of shape (n, d) whereas \n",
    "        n is the number of samples in df and \n",
    "        d is the output dimensionality of the embedding_fn\n",
    "    \"\"\"\n",
    "    vectorized_qc = np.zeros(shape=(df.shape[0], 300*3))\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        q_v = embedding_fn(row[\"question_vectors\"])\n",
    "        c_v = embedding_fn(row[\"comment_vectors\"])\n",
    "        s_v = embedding_fn(row[\"subject_vectors\"])\n",
    "        vectorized_qc[i] = np.concatenate((q_v,c_v,s_v))\n",
    "    \n",
    "    return vectorized_qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_fn_averaging_question_comment_subject(df, embedding_fn):\n",
    "    \"\"\"\n",
    "    Uses the embedding_fn to create d-dimensional embeddings from the tokens \n",
    "    of the questions, comments and subject respectively, and averaging these embeddings.\n",
    "    :param df\n",
    "        Dataframe consisting of multiple samples. Features will be computed for each sample individually\n",
    "    :param embedding_fn\n",
    "        As in 2a)\n",
    "    :returns\n",
    "        Matrix of shape (n, d) whereas \n",
    "        n is the number of samples in df and \n",
    "        d is the output dimensionality of the embedding_fn\n",
    "    \"\"\"\n",
    "    vectorized_qc = np.zeros(shape=(df.shape[0], 300*2))\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        q_v = embedding_fn(row[\"question_vectors\"])\n",
    "        c_v = embedding_fn(row[\"comment_vectors\"])\n",
    "        s_v = embedding_fn(row[\"subject_vectors\"])\n",
    "        vectorized_qc[i] = np.mean([q_v,c_v,s_v])\n",
    "    \n",
    "    return vectorized_qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: max-pooling ; Features: concat-question-comment\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.61      0.21      0.31        92\n",
      "        Good       0.81      0.96      0.88       322\n",
      "\n",
      "    accuracy                           0.79       414\n",
      "   macro avg       0.71      0.58      0.59       414\n",
      "weighted avg       0.77      0.79      0.75       414\n",
      "\n",
      "\n",
      "Embeddings: max-pooling ; Features: averaging-question-comment\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.33      0.04      0.08        92\n",
      "        Good       0.78      0.98      0.87       322\n",
      "\n",
      "    accuracy                           0.77       414\n",
      "   macro avg       0.56      0.51      0.47       414\n",
      "weighted avg       0.68      0.77      0.69       414\n",
      "\n",
      "\n",
      "Embeddings: max-pooling ; Features: concatenate-question-comment-subject\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.67      0.07      0.12        92\n",
      "        Good       0.79      0.99      0.88       322\n",
      "\n",
      "    accuracy                           0.79       414\n",
      "   macro avg       0.73      0.53      0.50       414\n",
      "weighted avg       0.76      0.79      0.71       414\n",
      "\n",
      "\n",
      "Embeddings: max-pooling ; Features: averaging-question-comment-subject\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.44      0.08      0.13        92\n",
      "        Good       0.79      0.97      0.87       322\n",
      "\n",
      "    accuracy                           0.77       414\n",
      "   macro avg       0.61      0.52      0.50       414\n",
      "weighted avg       0.71      0.77      0.71       414\n",
      "\n",
      "\n",
      "Embeddings: averaging ; Features: concat-question-comment\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.73      0.35      0.47        92\n",
      "        Good       0.84      0.96      0.90       322\n",
      "\n",
      "    accuracy                           0.83       414\n",
      "   macro avg       0.78      0.66      0.68       414\n",
      "weighted avg       0.81      0.83      0.80       414\n",
      "\n",
      "\n",
      "Embeddings: averaging ; Features: averaging-question-comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nguyen\\pycharmprojects\\nlp4web\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.00      0.00      0.00        92\n",
      "        Good       0.78      1.00      0.88       322\n",
      "\n",
      "    accuracy                           0.78       414\n",
      "   macro avg       0.39      0.50      0.44       414\n",
      "weighted avg       0.60      0.78      0.68       414\n",
      "\n",
      "\n",
      "Embeddings: averaging ; Features: concatenate-question-comment-subject\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.76      0.24      0.36        92\n",
      "        Good       0.82      0.98      0.89       322\n",
      "\n",
      "    accuracy                           0.81       414\n",
      "   macro avg       0.79      0.61      0.63       414\n",
      "weighted avg       0.80      0.81      0.77       414\n",
      "\n",
      "\n",
      "Embeddings: averaging ; Features: averaging-question-comment-subject\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.81      0.14      0.24        92\n",
      "        Good       0.80      0.99      0.89       322\n",
      "\n",
      "    accuracy                           0.80       414\n",
      "   macro avg       0.81      0.57      0.56       414\n",
      "weighted avg       0.80      0.80      0.74       414\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_functions = [\n",
    "    ('max-pooling', embedding_fn_max_pooling), \n",
    "    ('averaging', embedding_fn_average)\n",
    "]\n",
    "\n",
    "feature_functions = [\n",
    "    ('concat-question-comment', feature_fn_concatenate_question_comment),\n",
    "    (\"averaging-question-comment\",feature_fn_averaging_question_comment),\n",
    "    (\"concatenate-question-comment-subject\",feature_fn_concatenate_question_comment_subject),\n",
    "    (\"averaging-question-comment-subject\", feature_fn_averaging_question_comment_subject)\n",
    "]\n",
    "\n",
    "for embedding_name, embedding_fn in embedding_functions:\n",
    "    for feature_name, feature_fn in feature_functions:\n",
    "        print('Embeddings:', embedding_name, '; Features:', feature_name)\n",
    "        \n",
    "        # Adjust the naming of df_train and df_dev to match your training and dev set:\n",
    "        train_and_evaluate(train_data_, test_data_, embedding_fn, feature_fn)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
