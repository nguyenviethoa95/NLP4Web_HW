{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h1> NLP and the Web: Home Exercise 3 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>spaCy</b>: as discussed in class, this is a useful open-source library that enables the user to perform several NLP tasks with high quality results. It is not only helpful for beginners in NLP but also for advanced programmers, who want to integrate\n",
    "NLP features in real products. Given that we have been working from scratch with Python data structures and that it is\n",
    "not allowed to use any other libraries other than internal packages from Python, for this exercise you should only use\n",
    "spaCy, and Numpy and Pandas if needed. Please follow the instructions as given below and in case of questions use our\n",
    "Discussion forum in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Information\n",
    "We try to make the description of the parameters as clear as possible. However, if you think that there is some missing information please ask in Moodle.\n",
    "Furthermore we've decided to not name exact datatypes for the input and output parameters. We will use the term \"array like object\". This contains lists, dictionaries, sets, dataframes, etc. If you see this term you can decide which datatype you want to use. In general we think that using pandas is beneficial, so we prefer this option, but for this task it will not have any impact on grading if you do not use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - 2 Points\n",
    "For this task you have to use the dataset from the file yelp_polarity.txt. This file has two columns which are separated by '\\t'. The first column contains reviews and the second column contains a sentiment label per review (0=negative, 1=positive). First of all you have to read this file.\n",
    "\n",
    "<b>Hint</b>: For this task you have to use some spaCy functions. You can find some useful information about spaCy tokens and their attributes <a href=\"https://spacy.io/api/token\">here</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Tokenize every sentence in the file yelp_polarity.txt  (please ignore class labeling \"1\"/\"0\" in the document). Use spaCy to solve this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Implement the function `occurence_lowercase`. It shall calculate the (absolute) occurrence of each token that is lowercased. Then print the result. Do not lowercase all tokens, it's about identifiying and counting all lowercased tokens. Use spaCy to identify lowercased tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurence_lowercase(data):\n",
    "    \"\"\"\n",
    "    counts occurence of all lowercased tokens\n",
    "    @param data: array-like object containing the tokenized sentences from yelp_polarity.txt\n",
    "    @return: array-like object with tokens and their counts\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Implement the function `occurence_no_punctuation`. It shall extract all tokens with occurrences greater or equal 5, excluding punctuation. Additionally it shall return the absolut occurence of these tokens. Do not convert the case, just count the tokens as they are. Use spaCy to identify which tokens are considered as punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurence_no_punctuation(data):\n",
    "    \"\"\"\n",
    "    counts occurence of all tokens excluding punctuation and prints all with occurences greater or equal 5\n",
    "    @param data: array-like object containing the tokenized sentences from yelp_polarity.txt\n",
    "    @return: array-like object with tokens and their counts\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d) Use the token \"lecture\" to give an example to explain the internal structure of spaCy (Explain the three components - Please refer to Jupyter Notebook  from the practice class). Use at most 5 sentences. Additionally you can check  <a href=\"https://spacy.io/api\">this link</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - 4 Points\n",
    "Use the code of Task 1 to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Explore at least 5 lexical attributes of the top five most frequent tokens from task 1c) (other than lemma, pos-tag, dependency), and explain each of them with up to two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) After observing relevant attributes, write (a) pattern(s) to create groups of similar tokens (itâ€™s up to you which similarity metric/criteria you use). Use the matcher to define the pattern. After this use this pattern to extract and print the results from the file yelp_polartity.txt. The output should include the original sentence and a list of the extracted tokens. Explain your answer and discuss the reason for the chosen pattern in up to two sentences.\n",
    "\n",
    "<b>Hint</b>: You can check this <a href=\"https://explosion.ai/demos/matcher\">link</a>\n",
    "to try out different patterns<br>\n",
    "<b>Example 1</b>: All tokens that describe a date or time -> Let's meet this evening: <b>this evening</b> <br>\n",
    "<b>Example 2</b>: Group all tokens that are verbs in the past-> I ate pizza: <b>ate</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Briefly describe another use case which you could have implemented in up to two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - 4 Points\n",
    "Using the whole document (yelp_polarity.txt):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a)  Implement the function `proper_nouns`. It shall use the matcher/ patterns to extract and print proper nouns with more than one token. The output should contain the original sentence and the identified proper nouns with more than one token. If there was no match in a sentence do not print it.\n",
    "\n",
    "<b>Hint 1</b>: If there is a proper noun like \"New York City\" you should only print \"New York City\" and not \"New York\" \"New York City\" \"York City\". Additionally you can check this <a href=\"https://explosion.ai/demos/matcher\">link</a>\n",
    "to try out different patterns.\n",
    "<br><br>\n",
    "<b>Hint 2</b>: For this task you may require some functions which are not provided by spaCy.\n",
    "<br> <br>\n",
    "<b>Example</b>: The sentence \"I live in New York City and I like Hot Dogs & Coke\" should look like this in the output:\n",
    "<br>\n",
    "I live in New York City and I like Hot Dogs & Coke<br>\n",
    "  -New York City<br>\n",
    "  -Hot Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_nouns(data):\n",
    "    \"\"\"\n",
    "    Prints all proper nouns with more than one token\n",
    "    @param data: array-like object containing the tokenized sentences from yelp_polarity.txt\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b)  Additionally, implement the function `shared_lemmas`. It shall figure out which verbs and nouns (including proper nouns) share the same lemma. The function should print each lemma exactly once together with all lowercased, distinct, non-lemmatized nouns and verbs from the text that share this lemma (e.g. lemma: walk; nouns: walk; verbs: walk, walking, walked). \n",
    "\n",
    "<b>Hint</b>: For this task you may require some functions which are not provided by spaCy. For example you can join two dataframes and group strings by concatenating them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_lemmas(data):\n",
    "    \"\"\"\n",
    "    Prints all nouns and verbs which share th same lemmas\n",
    "        Additionally all verbs and nouns are converted to lowercase\n",
    "    @param data: array-like object containing the tokenized sentences from yelp_polarity.txt\n",
    "    \"\"\"\n",
    "    # get unique verbs and nouns with their respective lemmas\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please upload in Moodle your working Jupyter-Notebook <b>before next lab session</b> <span style=\"color:red\">(Nov 26st, 4:14pm)</span>. Submission format: ExerciseX_YourName.zip<br>\n",
    "Submission should contain your filled out Jupyter notebook template (naming schema: ExerciseX_YourName.ipynb) and any auxiliar files that are necessary to run your code (e.g. datasets provided by us)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
