{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and the Web: Home Exercise 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification is the task of categorizing text data into a set of predefined labels. The most important part of text classification is feature engineering: the process of extracting features from raw text data for machine learning models. We discussed previously what is a bag of words (bow) and why is it important to use it. In today’s class, we have seen how to transform a raw text into a set of features that can be represented as a matrix or a vector. In this exercise, we will practice feature engineering for text classification with <a href=\"https://scikit-learn.org/stable/\">scikit-learn</a>. The goal of this exercise is to explore different features and their representations, train and evaluate different classifiers to automatically identify the sentiment polarity of the movie reviews.<br><br>\n",
    "<b>Data:</b> The dataset provided for this exercise contains 5k movie reviews with positive and negative labels, which were taken from IMDB Dataset. It's saved in the file *IMDB_reviews.csv*, which has two columns separated by ','. The first column contains reviews and the second column contains their sentiment labels (0=negative, 1=positive). <br><br>\n",
    "<b>Note:</b> For this exercise, you may only use spaCy, scikit-learn, NumPy, Pandas and internal packages from Python. Please follow the instructions as given below and in case of questions use our Discussion forum in Moodle, we don’t answer questions via email.<br><br>\n",
    "Please use comments where appropriate to help tutors understand your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - 5 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Read the data from *IMDB_reviews.csv*. Shuffle and split it into training (60%), development (20%) and test(20%) sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training/dev/test samples 3000 - 1000 - 1000: \n"
     ]
    }
   ],
   "source": [
    "reviews_df = pd.read_csv(\"IMDB_reviews.csv\",sep=\",\",header=0)\n",
    "X_train_dev, X_test, y_train_dev, y_test = train_test_split(reviews_df.iloc[:,0], reviews_df.iloc[:,1], test_size=0.2, random_state=5,shuffle=True)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train_dev, y_train_dev, test_size=0.25, random_state=5)\n",
    "print(\"Number of training/dev/test samples {0} - {1} - {2}: \".format(X_train.shape[0],X_dev.shape[0],X_test.shape[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Use the CountVectorizer() and the MultinomialNB() provided by scikit-learn, train two multinomial Naive Bayes classifiers with the training set. One classifier uses the count matrix (absolute occurrence of each word) and another one uses the binary count matrix (binary, whether a word occurs in a text) as features. Evaluate them on the development set, print the accuracy.  \n",
    "**Requirement:** For the reusability of subsequent tasks, you should first implement the function `train_valid_cls`, and then call the function to train and evaluate the classifiers. The required input parameters are described as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train_valid_cls(train_texts, train_labels, dev_texts, dev_labels, vectorizer, classifier):\n",
    "    \"\"\"\n",
    "    Train and validate the classifier with the given data and vectorizer, print the accuracy of \n",
    "    the trained classifier on the development set.\n",
    "    \n",
    "    @param train_texts: array-like object containing review texts from the training set\n",
    "    @param train_labels: array-like object with corresponding sentiment labels for train_texts\n",
    "    @param dev_texts: array-like object containing review texts from the development set\n",
    "    @param dev_labels: array-like object with corresponding sentiment labels for dev_texts\n",
    "    @param vectorizer: a customized scikit-learn Vectorizer \n",
    "    @param classifier: a scikit-learn Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(train_texts)\n",
    "\n",
    "    # encode document\n",
    "    occ_train_vector = vectorizer.transform(train_texts)   \n",
    "    occ_dev_vector = vectorizer.transform(dev_texts)\n",
    "    \n",
    "    #bin_train_vector = occ_train_vector.toarray()\n",
    "    \n",
    "    # train\n",
    "    count_acc = classifier.fit(occ_train_vector, train_labels)\n",
    "    \n",
    "    # Predict new observation's class\n",
    "    y_hat = classifier.predict(occ_dev_vector)\n",
    "    count_acc = accuracy_score(dev_labels,y_hat)\n",
    "    \n",
    "    return count_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MultinomialNB with occurence as feature:  0.829\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "clf = MultinomialNB()\n",
    "acc = train_valid_cls(X_train, y_train, X_dev, y_dev, vectorizer, clf)\n",
    "print(\"Accuracy for MultinomialNB with occurence as feature: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_cls_bin(train_texts, train_labels, dev_texts, dev_labels, vectorizer, classifier):\n",
    "    \"\"\"\n",
    "    Train and validate the classifier with the given data and vectorizer, print the accuracy of \n",
    "    the trained classifier on the development set.\n",
    "    \n",
    "    @param train_texts: array-like object containing review texts from the training set\n",
    "    @param train_labels: array-like object with corresponding sentiment labels for train_texts\n",
    "    @param dev_texts: array-like object containing review texts from the development set\n",
    "    @param dev_labels: array-like object with corresponding sentiment labels for dev_texts\n",
    "    @param vectorizer: a customized scikit-learn Vectorizer \n",
    "    @param classifier: a scikit-learn Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(train_texts)\n",
    "\n",
    "    # encode document\n",
    "    occ_train_vector = vectorizer.transform(train_texts)   \n",
    "    occ_dev_vector = vectorizer.transform(dev_texts)\n",
    "    \n",
    "    bin_train_vector = occ_train_vector.toarray()\n",
    "    bin_dev_vector = occ_dev_vector.toarray()\n",
    "    \n",
    "    # train and test on dev set \n",
    "    y_hat = classifier.fit(bin_train_vector, train_labels).predict(bin_dev_vector)\n",
    "    \n",
    "    # Calculate the accuracy on dev set \n",
    "    count_acc = accuracy_score(dev_labels,y_hat)\n",
    "    \n",
    "    return count_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MultinomialNB with one hot encoding as features:  0.829\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "clf = MultinomialNB()\n",
    "acc_bin = train_valid_cls_bin(X_train, y_train, X_dev, y_dev, vectorizer, clf)\n",
    "print(\"Accuracy for MultinomialNB with one hot encoding as features: \", acc_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Create a list of at least 20 (stop) words that you think are useless for the training (and removing them could help improve the accuracy of the classifier). Call the function `train_valid_cls` from b), train and validate two new multinomial Naive Bayes classifiers using the count matrix and binary count matrix **without these (stop) words**. Compare the results with b), and briefly explain why you think this can improve the accuracy (and analyse the possible reason if it doesn't work as you expect) in 2-4 sentences.  \n",
    "**Hint:** Check <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">here</a> may help you find an easy way to remove the stop words. You are allowed to take a look at existing stop word lists, but please choose the stop words with your own consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MultinomialNB with occurence as feature without stopwords:  0.835\n"
     ]
    }
   ],
   "source": [
    "vectorizer_without_stopwords = CountVectorizer(stop_words=\"english\")\n",
    "clf = MultinomialNB()\n",
    "acc_without_stop = train_valid_cls(X_train,y_train,X_dev,y_dev,vectorizer_without_stopwords, classifier)\n",
    "print(\"Accuracy for MultinomialNB with occurence as feature without stopwords: \", acc_without_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MultinomialNB with one hot encoding as features without stopwords:  0.835\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "clf = MultinomialNB()\n",
    "acc_bin = train_valid_cls_bin(X_train, y_train, X_dev, y_dev, vectorizer_without_stopwords, clf)\n",
    "print(\"Accuracy for MultinomialNB with one hot encoding as features without stopwords: \", acc_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Explore at least 3 different ranges of n-grams (introduce bigram, trigram ... features) and try to find the best one for training the multinomial Naive Bayes classifier with count matirx or binary count matrix. Report the accuracy on the development set for every range you tried. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MultinomialNB with bigram as features without stopwords:  0.842\n",
      "Accuracy for MultinomialNB with trigram as features without stopwords:  0.846\n",
      "0.842\n"
     ]
    }
   ],
   "source": [
    "# 2-gram\n",
    "bi_gram_vec = CountVectorizer(stop_words=\"english\", analyzer='word', ngram_range=(1, 2))\n",
    "classifier = MultinomialNB()\n",
    "bi_gram_acc = train_valid_cls(X_train,y_train,X_dev,y_dev,bi_gram_vec, classifier)\n",
    "print(\"Accuracy for MultinomialNB with bigram as features without stopwords: \", bi_gram_acc)\n",
    "\n",
    "# 3-gram\n",
    "tri_gram_vec = CountVectorizer(stop_words=\"english\", analyzer='word', ngram_range=(1, 3))\n",
    "classifier = MultinomialNB()\n",
    "tri_gram_acc = train_valid_cls(X_train,y_train,X_dev,y_dev,tri_gram_vec, classifier)\n",
    "print(\"Accuracy for MultinomialNB with trigram as features without stopwords: \", tri_gram_acc)\n",
    "\n",
    "# ignore terms that appear in less than 1 document\n",
    "min_df_vec = CountVectorizer(stop_words=\"english\", analyzer='word', ngram_range=(1, 2),min_df = 1)\n",
    "classifier = MultinomialNB()\n",
    "min_df_acc = train_valid_cls(X_train,y_train,X_dev,y_dev,min_df_vec, classifier)\n",
    "print(min_df_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - 5 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Tokenize every review text in the training and development sets using spaCy (It may take a few minutes). Filter out tokens except verbs, adjectives and adverbs. You should store the remaining tokens as spaCy token objects (instead of strings) for the subsequent tasks b) and c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "filtered_reviews_train = []\n",
    "filtered_reviews_dev = []\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "whitelist = [\"VERB\",\"ADJ\",\"ADV\"]\n",
    "\n",
    "for doc in X_train:\n",
    "    tokens = nlp(doc)\n",
    "    temp = []\n",
    "    for token in tokens:        \n",
    "        if token.pos_ in whitelist:\n",
    "            temp.append(token)\n",
    "    filtered_reviews_train.append(temp)\n",
    "    \n",
    "for doc in X_dev:\n",
    "    tokens = nlp(doc)\n",
    "    temp = []\n",
    "    for token in tokens:        \n",
    "        if token.pos_ in whitelist:\n",
    "            temp.append(token)\n",
    "    filtered_reviews_dev.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Lemmatize the remaining tokens from a). Train and validate a new multinomial Naive Bayes classifier with the count matrix of the lemmatized tokens, print the accuracy on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "lemmatized_reviews_train = []\n",
    "lemmatized_reviews_dev = []\n",
    "\n",
    "for reviews in filtered_reviews_train:\n",
    "    temp=[]\n",
    "    for token in reviews: \n",
    "        temp.append(token.lemma_)\n",
    "    lemmatized_reviews_train.append(temp)\n",
    "    \n",
    "for i, v in enumerate(lemmatized_reviews_train):\n",
    "    lemmatized_reviews_train[i]=\" \".join(v)\n",
    "    \n",
    "for reviews in filtered_reviews_dev:\n",
    "    temp=[]\n",
    "    for token in reviews: \n",
    "        temp.append(token.lemma_)\n",
    "    lemmatized_reviews_dev.append(temp)\n",
    "    \n",
    "for i, v in enumerate(lemmatized_reviews_dev):\n",
    "    lemmatized_reviews_dev[i]=\" \".join(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MultinomialNB with occurance as features and lemmatized words:  0.857\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "clf = MultinomialNB()\n",
    "lemmatized_acc = train_valid_cls(lemmatized_reviews_train, y_train, lemmatized_reviews_dev, y_dev, vectorizer, clf)\n",
    "print(\"Accuracy for MultinomialNB with occurance as features and lemmatized words: \",lemmatized_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Extract the spaCy word vectors of the remaining tokens from a). For each review text, calculate the average of all word vectors as its vector representation. Then train a gaussian Naive Bayes classifier (sklearn.naive_bayes.GaussianNB) and a linear SVM classifier (sklearn.svm.LinearSVC) with the obtained vector representations, evaluate and print their accuracy on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(filtered_reviews_train):\n",
    "    filtered_reviews_train[i] = \" \".join( [t.text for t in filtered_reviews_train[i]] )\n",
    "\n",
    "for i, v in enumerate(filtered_reviews_dev):\n",
    "    filtered_reviews_dev[i] = \" \".join( [t.text for t in filtered_reviews_dev[i]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np \n",
    "\n",
    "clf_svc = LinearSVC()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(filtered_reviews_train)\n",
    "\n",
    "# encode document\n",
    "filtered_train_vector = vectorizer.transform(filtered_reviews_train)   \n",
    "filtered_dev_vector = vectorizer.transform(filtered_reviews_dev)\n",
    "\n",
    "# create word vector \n",
    "average_train = np.sum(filtered_train_vector, axis=1)\n",
    "average_dev = np.sum(filtered_dev_vector, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\hw\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.522\n"
     ]
    }
   ],
   "source": [
    "## train\n",
    "clf_svc.fit(average_train, y_train)\n",
    "\n",
    "## Predict new observation's class\n",
    "y_hat = clf_svc.predict(average_dev)\n",
    "svc_average_acc = accuracy_score(y_dev,y_hat)\n",
    "print(svc_average_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmprojects\\hw\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB()\n",
    "\n",
    "## train and predict\n",
    "y_hat = clf_svc.fit(average_train, y_train).predict(average_dev)\n",
    "\n",
    "## Calculate the result\n",
    "gnb_average_acc = accuracy_score(y_dev,y_hat)\n",
    "print(gnb_average_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Explain the general differences between discriminative and generative models in 2-3 sentences, and identify the corresponding model category for the two classifiers from c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A discriminative model e.g. support vector machine models the decision boundary between the classes. A generative model e.g. naives bayes classifier explicitly models the actual distribution of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Choose your best model from the whole exercise and test it on the test set, print the accuracy. Why is it important to evaluate your final model on a previously unused test set? Explain it in up to 2 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on a separated test set gives an accurate evaluation of the performance of the model, since the model should be tested on the data that it has never seen before in the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "filtered_reviews_test = []\n",
    "\n",
    "for doc in X_test:\n",
    "    tokens = nlp(doc)\n",
    "    temp = []\n",
    "    for token in tokens:        \n",
    "        if token.pos_ in whitelist:\n",
    "            temp.append(token)\n",
    "    filtered_reviews_test.append(temp)\n",
    "\n",
    "lemmatized_reviews_test = []\n",
    "for reviews in filtered_reviews_test:\n",
    "    temp=[]\n",
    "    for token in reviews: \n",
    "        temp.append(token.lemma_)\n",
    "    lemmatized_reviews_test.append(temp)\n",
    "    \n",
    "for i, v in enumerate(lemmatized_reviews_test):\n",
    "    lemmatized_reviews_test[i]=\" \".join(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MultinomialNB with occurance as features and lemmatized words without stopwords:  0.827\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "clf_final = MultinomialNB()\n",
    "lemmatized_acc = train_valid_cls(lemmatized_reviews_train, y_train, lemmatized_reviews_dev, y_dev, vectorizer, clf_final)\n",
    "occ_test_vector = vectorizer.transform(lemmatized_reviews_test)\n",
    "y_hat = clf_final.predict(occ_test_vector)\n",
    "final_acc = accuracy_score(y_test,y_hat)\n",
    "\n",
    "print(\"Accuracy for MultinomialNB with occurance as features and lemmatized words without stopwords: \",final_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please upload in Moodle your working Jupyter-Notebook <b>before next lab session</b> <span style=\"color:red\">(Dec 3rd, 4:14pm)</span>. Submission format: ExerciseX_YourName.zip<br>\n",
    "Submission should contain your filled out Jupyter notebook template (naming schema: ExerciseX_YourName.ipynb) and any auxiliar files that are necessary to run your code (e.g. datasets provided by us)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
